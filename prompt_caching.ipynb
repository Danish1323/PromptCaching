{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64ac65f",
   "metadata": {},
   "source": [
    "### We are using a very useful tool here named litellm which is a very lightweight and helpful tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4ac766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Trying to study with an orange cat around is tough. Every time I say “focus,” he says “paws” and sits on my notes, turning my study session into a cat-astrophe I never planned."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from litellm import completion\n",
    "\n",
    "\n",
    "\n",
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student who is studying but is being disturbed by his orange cat\"},\n",
    "]\n",
    "\n",
    "response = completion(model=\"openai/gpt-5-nano\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633394b",
   "metadata": {},
   "source": [
    "Here above you can see how we get the response from openai chat.completions api and below using the litellm we can see the more in detail nitty and gritty stuff of this output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c9be9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 23\n",
      "Output tokens: 2686\n",
      "Total tokens: 2709\n",
      "Total cost: 0.1076 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d0815",
   "metadata": {},
   "source": [
    "### Now lets do soemthing very fun and useful, we will simulate a bit of rag like feature with litellm and also we will see how prompt caching works and how it helps reduce us the api costs!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0dec32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb5966fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In Shakespeare's *Hamlet*, when Laertes, distraught and enraged after hearing of his father's death, cries out, **\"Where is my father?\"**, the reply he receives is:\n",
       "\n",
       "**\"He is dead.\"**\n",
       "\n",
       "This stark and brutal reply is delivered by **Claudius**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]\n",
    "\n",
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a877cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 19\n",
      "Output tokens: 61\n",
      "Total tokens: 80\n",
      "Total cost: 0.0026 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73403600",
   "metadata": {},
   "source": [
    "#### See above how the answer was wrong , now lets use a little bit of rag kinda work around, but as we know so much context will take up a lot of money, then we will also see prompt caching as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "095b8a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks, \"Where is my father?\", the reply is given by **Claudius, the King of Denmark**.\n",
       "\n",
       "The specific reply is: **\"Dead.\"**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question[0][\"content\"] += \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet\n",
    "\n",
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7223cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 53208\n",
      "Output tokens: 36\n",
      "Cached tokens: None\n",
      "Total cost: 0.5335 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fce95b",
   "metadata": {},
   "source": [
    "#### The above cost is so much, about 1 cent for such a small answer!, now lets see the power of prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faf5ceaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"Where is my father?\", the reply is:\n",
       "\n",
       "**\"Dead.\"**\n",
       "\n",
       "This reply comes from Claudius, the King."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b3ee2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 53208\n",
      "Output tokens: 31\n",
      "Cached tokens: 52216\n",
      "Total cost: 0.0634 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Cached tokens: {response.usage.prompt_tokens_details.cached_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab70ab87",
   "metadata": {},
   "source": [
    "#### see how the cost lowered to about 10 times less, and as you can see the cached tokens is so much! thats the power of prompt caching!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294dd04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
